{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4fb3b69",
   "metadata": {},
   "source": [
    "# Feature Selection - Part 1\n",
    "## Understanding Feature Selection in Machine Learning\n",
    "\n",
    "In this notebook, we'll explore Feature Selection techniques and their implementation in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e04437",
   "metadata": {},
   "source": [
    "## What is Feature Selection?\n",
    "\n",
    "Feature Selection is the process of automatically selecting the most relevant features (variables, predictors) for your model. It's a crucial step in the machine learning pipeline that:\n",
    "\n",
    "1. Identifies the most important features\n",
    "2. Removes irrelevant or redundant features\n",
    "3. Reduces the dimensionality of your dataset\n",
    "4. Improves model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd23c06",
   "metadata": {},
   "source": [
    "## Why do we need Feature Selection?\n",
    "\n",
    "Feature Selection is important for several reasons:\n",
    "\n",
    "1. **Reduces Overfitting**: Fewer redundant features means less opportunity to make decisions based on noise\n",
    "2. **Improves Accuracy**: Less misleading data means model accuracy improves\n",
    "3. **Reduces Training Time**: Fewer features mean faster training\n",
    "4. **Enhanced Generalization**: Reduces variance and helps avoid overfitting\n",
    "5. **Better Interpretability**: Fewer features make it easier to explain the model's decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ffc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, chi2\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"\\nFeature names:\\n\", data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940a147",
   "metadata": {},
   "source": [
    "## Types of Feature Selection\n",
    "\n",
    "There are three main types of feature selection methods:\n",
    "\n",
    "1. **Filter Methods**: Use statistical measures to score features\n",
    "2. **Wrapper Methods**: Use model performance to evaluate feature subsets\n",
    "3. **Embedded Methods**: Perform feature selection during model training\n",
    "\n",
    "In this notebook, we focus on Filter Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3324f81",
   "metadata": {},
   "source": [
    "## Filter-based Feature Selection\n",
    "\n",
    "Filter methods use statistical measures to score the correlation or dependence between input variables that can be filtered to choose the most relevant features. These methods are:\n",
    "- Computationally fast\n",
    "- Independent of the learning algorithm\n",
    "- Usually univariate\n",
    "\n",
    "Let's explore different filter methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f970f",
   "metadata": {},
   "source": [
    "### 1. Duplicate Features\n",
    "\n",
    "First, let's check for duplicate features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate features\n",
    "def find_duplicates(df):\n",
    "    duplicates = {}\n",
    "    columns = df.columns\n",
    "    \n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i+1, len(columns)):\n",
    "            if df[columns[i]].equals(df[columns[j]]):\n",
    "                duplicates[columns[j]] = columns[i]\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "duplicates = find_duplicates(X)\n",
    "print(\"Duplicate features:\", duplicates if duplicates else \"None found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045e4d6",
   "metadata": {},
   "source": [
    "### 2. Variance Threshold\n",
    "\n",
    "Variance Threshold is a simple baseline approach to feature selection. It removes features whose variance doesn't meet a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676dd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Variance Threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_var_selected = selector.fit_transform(X_scaled)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "print(\"Original number of features:\", X.shape[1])\n",
    "print(\"Number of selected features:\", len(selected_features))\n",
    "print(\"\\nSelected features:\\n\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ac3bc",
   "metadata": {},
   "source": [
    "### 3. Correlation Analysis\n",
    "\n",
    "Correlation analysis helps identify redundant features by measuring the linear correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated features\n",
    "def find_high_correlations(correlation_matrix, threshold=0.9):\n",
    "    high_corr = np.where(np.abs(correlation_matrix) > threshold)\n",
    "    high_corr = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y]) \n",
    "                 for x, y in zip(*high_corr) if x != y and x < y]\n",
    "    return pd.DataFrame(high_corr, columns=['Feature 1', 'Feature 2', 'Correlation'])\n",
    "\n",
    "high_correlations = find_high_correlations(correlation_matrix)\n",
    "print(\"\\nHighly correlated features (>0.9):\\n\")\n",
    "print(high_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cceb2e",
   "metadata": {},
   "source": [
    "### 4. ANOVA (Analysis of Variance)\n",
    "\n",
    "ANOVA is used to determine the statistical significance of features for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ANOVA F-test\n",
    "f_scores, p_values = f_classif(X_scaled, y)\n",
    "\n",
    "# Create DataFrame with feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F Score': f_scores,\n",
    "    'P Value': p_values\n",
    "})\n",
    "\n",
    "# Sort by F-score\n",
    "feature_scores = feature_scores.sort_values('F Score', ascending=False)\n",
    "print(\"Top 10 features by ANOVA F-score:\\n\")\n",
    "print(feature_scores.head(10))\n",
    "\n",
    "# Plot F-scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(f_scores)), f_scores)\n",
    "plt.xticks(range(len(f_scores)), X.columns, rotation=90)\n",
    "plt.title('ANOVA F-scores for each feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea3f91",
   "metadata": {},
   "source": [
    "### 5. Chi-Square Test\n",
    "\n",
    "Chi-Square test is another statistical test for feature selection, particularly useful for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Chi-Square test\n",
    "# Note: Chi-square requires non-negative values, so we use MinMaxScaler\n",
    "X_scaled_positive = MinMaxScaler().fit_transform(X)\n",
    "chi_scores, chi_p_values = chi2(X_scaled_positive, y)\n",
    "\n",
    "# Create DataFrame with feature scores\n",
    "chi_feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Chi-Square Score': chi_scores,\n",
    "    'P Value': chi_p_values\n",
    "})\n",
    "\n",
    "# Sort by Chi-Square score\n",
    "chi_feature_scores = chi_feature_scores.sort_values('Chi-Square Score', ascending=False)\n",
    "print(\"Top 10 features by Chi-Square score:\\n\")\n",
    "print(chi_feature_scores.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd91ff",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Filter Methods\n",
    "\n",
    "### Advantages:\n",
    "1. **Fast**: Computationally efficient\n",
    "2. **Scalable**: Can handle large datasets\n",
    "3. **Independent**: Works independently of the learning algorithm\n",
    "4. **Univariate**: Each feature is considered independently\n",
    "\n",
    "### Disadvantages:\n",
    "1. **Ignores Feature Dependencies**: Doesn't account for feature interactions\n",
    "2. **Ignores Learning Algorithm**: May select features that aren't optimal for the specific model\n",
    "3. **Redundancy**: May select redundant features (unless explicitly checked)\n",
    "4. **Arbitrary Thresholds**: Often requires manual threshold setting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
