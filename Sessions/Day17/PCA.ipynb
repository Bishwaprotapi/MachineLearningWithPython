{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) Tutorial\n",
    "\n",
    "This notebook covers the following topics:\n",
    "1. Introduction to Dimensionality Reduction and PCA\n",
    "2. Using Scikit-Learn's PCA implementation\n",
    "3. Understanding PCA Variance Ratio\n",
    "4. Randomized PCA\n",
    "5. Incremental PCA\n",
    "6. PCA on the MNIST dataset\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, RandomizedPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_openml, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Dimensionality Reduction and PCA\n",
    "\n",
    "### What is Dimensionality Reduction?\n",
    "\n",
    "Dimensionality reduction refers to techniques that reduce the number of features in a dataset while preserving as much information as possible. These techniques are useful for:\n",
    "\n",
    "- Reducing computational complexity\n",
    "- Visualizing high-dimensional data\n",
    "- Removing redundant or correlated features\n",
    "- Reducing overfitting by simplifying the model\n",
    "- Addressing the \"curse of dimensionality\"\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "Principal Component Analysis (PCA) is one of the most popular dimensionality reduction techniques. It works by:\n",
    "\n",
    "1. Finding directions (principal components) in the data that maximize variance\n",
    "2. Projecting the data onto these principal components\n",
    "3. Using only the top N components to represent the data in lower dimensions\n",
    "\n",
    "The principal components are orthogonal to each other, and they are ordered by the amount of variance they explain in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate PCA with a simple 2D dataset first, so we can visualize what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple 2D dataset with correlation\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Create correlated data\n",
    "X = np.random.randn(n_samples, 2)\n",
    "transformation = [[0.8, -0.6], [0.6, 0.8]]\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.7)\n",
    "plt.title('Original 2D Dataset with Correlation')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to find principal components\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# Get the principal components\n",
    "components = pca.components_\n",
    "explained_variance = pca.explained_variance_\n",
    "\n",
    "# Plot the data and principal components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.7)\n",
    "\n",
    "# Plot the principal components\n",
    "for i, (component, variance) in enumerate(zip(components, explained_variance)):\n",
    "    plt.arrow(0, 0, component[0] * np.sqrt(variance), component[1] * np.sqrt(variance),\n",
    "              head_width=0.1, head_length=0.1, fc=f'C{i+2}', ec=f'C{i+2}',\n",
    "              label=f'PC{i+1} (var={variance:.2f})')\n",
    "\n",
    "plt.title('Dataset with Principal Components')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {np.sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Scikit-Learn's PCA Implementation\n",
    "\n",
    "Scikit-Learn makes it easy to apply PCA to your data. Let's walk through the standard workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex dataset\n",
    "X, _ = make_blobs(n_samples=1000, n_features=10, centers=5, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=3)  # Reduce to 3 dimensions\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Reduced shape: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transformed data (first 2 principal components)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n",
    "plt.title('Data projected onto first two principal components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Parameters of Scikit-Learn's PCA\n",
    "\n",
    "- `n_components`: Number of components to keep\n",
    "  - Integer value: Keep specific number of components\n",
    "  - Float value between 0 and 1: Keep enough components to explain that ratio of variance\n",
    "  - 'mle': Use automatic dimensionality selection\n",
    "- `svd_solver`: Method for SVD calculation\n",
    "  - 'auto': Auto-select based on data size\n",
    "  - 'full': Run exact full SVD\n",
    "  - 'arpack': Run randomized SVD\n",
    "  - 'randomized': Use randomized algorithm\n",
    "- `whiten`: Boolean, whether to whiten the data (normalize components to have unit variance)\n",
    "- `random_state`: Controls the randomization in the algorithm (if applicable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding PCA Variance Ratio\n",
    "\n",
    "The explained variance ratio tells us how much of the total variance in the dataset is explained by each principal component. It's a key metric for determining how many components to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA without limiting components\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Individual explained variance\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1))\n",
    "\n",
    "# Cumulative explained variance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1))\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% Variance')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of components to explain 95% of variance\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "print(f\"Number of components needed to explain 95% of variance: {n_components_95}\")\n",
    "\n",
    "# Apply PCA with variance ratio\n",
    "pca_95 = PCA(n_components=0.95)  # Keep components that explain 95% of variance\n",
    "X_pca_95 = pca_95.fit_transform(X_scaled)\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Reduced shape: {X_pca_95.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Explained Variance Ratio to Select Number of Components\n",
    "\n",
    "There are several strategies to select the number of components:\n",
    "\n",
    "1. **Explained Variance Threshold**: Choose components that explain a certain percentage of variance (e.g., 95%).\n",
    "2. **Elbow Method**: Look for the \"elbow\" in the plot, where adding more components results in diminishing returns.\n",
    "3. **Kaiser Rule**: Keep components with eigenvalues (or explained variance) greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Randomized PCA\n",
    "\n",
    "For large datasets, computing the full PCA can be computationally expensive. Randomized PCA uses approximation techniques to speed up the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In newer scikit-learn versions, RandomizedPCA is deprecated\n",
    "# Instead, use PCA with svd_solver='randomized'\n",
    "\n",
    "# Generate a larger dataset\n",
    "X_large, _ = make_blobs(n_samples=10000, n_features=100, centers=10, random_state=42)\n",
    "X_large_scaled = StandardScaler().fit_transform(X_large)\n",
    "\n",
    "# Time the standard PCA\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "pca_full = PCA(n_components=10, svd_solver='full')\n",
    "X_pca_full = pca_full.fit_transform(X_large_scaled)\n",
    "full_time = time.time() - start_time\n",
    "print(f\"Full PCA time: {full_time:.4f} seconds\")\n",
    "\n",
    "# Time the randomized PCA\n",
    "start_time = time.time()\n",
    "pca_random = PCA(n_components=10, svd_solver='randomized', random_state=42)\n",
    "X_pca_random = pca_random.fit_transform(X_large_scaled)\n",
    "random_time = time.time() - start_time\n",
    "print(f\"Randomized PCA time: {random_time:.4f} seconds\")\n",
    "print(f\"Speedup: {full_time/random_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare variance explained\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, 11), pca_full.explained_variance_ratio_, alpha=0.5, label='Full PCA')\n",
    "plt.bar(range(1, 11), pca_random.explained_variance_ratio_, alpha=0.5, label='Randomized PCA')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Full PCA vs Randomized PCA: Explained Variance')\n",
    "plt.legend()\n",
    "plt.xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Randomized PCA\n",
    "\n",
    "- For large datasets (many samples or features)\n",
    "- When computation time is a concern\n",
    "- When you need only a few principal components\n",
    "- When an approximate solution is acceptable\n",
    "\n",
    "Key parameters for randomized PCA:\n",
    "- `n_components`: Number of components\n",
    "- `random_state`: Controls randomization for reproducibility\n",
    "- `iterated_power`: Number of power iterations (higher values improve accuracy but take longer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Incremental PCA\n",
    "\n",
    "Incremental PCA (IPCA) is designed for datasets that are too large to fit in memory. It processes the data in batches, making it suitable for online learning or large-scale applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a large dataset using batches\n",
    "X_huge, _ = make_blobs(n_samples=20000, n_features=50, centers=15, random_state=42)\n",
    "X_huge_scaled = StandardScaler().fit_transform(X_huge)\n",
    "\n",
    "# Apply Incremental PCA\n",
    "n_batches = 10\n",
    "batch_size = X_huge.shape[0] // n_batches\n",
    "\n",
    "ipca = IncrementalPCA(n_components=10)\n",
    "\n",
    "# Process data in batches\n",
    "start_time = time.time()\n",
    "for i in range(n_batches):\n",
    "    batch = X_huge_scaled[i * batch_size:(i + 1) * batch_size]\n",
    "    ipca.partial_fit(batch)\n",
    "\n",
    "ipca_time = time.time() - start_time\n",
    "print(f\"Incremental PCA time: {ipca_time:.4f} seconds\")\n",
    "\n",
    "# Transform the data\n",
    "X_ipca = ipca.transform(X_huge_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with regular PCA\n",
    "start_time = time.time()\n",
    "pca_regular = PCA(n_components=10)\n",
    "X_pca_regular = pca_regular.fit_transform(X_huge_scaled)\n",
    "regular_time = time.time() - start_time\n",
    "print(f\"Regular PCA time: {regular_time:.4f} seconds\")\n",
    "\n",
    "# Compare explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, 11), pca_regular.explained_variance_ratio_, alpha=0.5, label='Regular PCA')\n",
    "plt.bar(range(1, 11), ipca.explained_variance_ratio_, alpha=0.5, label='Incremental PCA')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Regular PCA vs Incremental PCA: Explained Variance')\n",
    "plt.legend()\n",
    "plt.xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Incremental PCA\n",
    "\n",
    "- For datasets too large to fit in memory\n",
    "- When data arrives in a streaming manner\n",
    "- In online learning scenarios\n",
    "- When working with very large datasets on machines with limited memory\n",
    "\n",
    "Key parameters for Incremental PCA:\n",
    "- `n_components`: Number of components\n",
    "- `batch_size`: Size of batches for the incremental fit\n",
    "- `whiten`: Whether to whiten the output\n",
    "\n",
    "The `partial_fit()` method allows for processing data in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA on the MNIST Dataset\n",
    "\n",
    "Now, let's apply PCA to a real-world dataset: MNIST handwritten digits. This demonstrates how PCA can be used for dimensionality reduction in image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X_mnist = mnist.data.astype('float32')\n",
    "y_mnist = mnist.target.astype('int')\n",
    "\n",
    "# Scale the data\n",
    "X_mnist_scaled = X_mnist / 255.0\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist_scaled, y_mnist, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"MNIST data shape: {X_mnist.shape}\")\n",
    "print(f\"Each image is a vector of length 784 (28x28 pixels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some MNIST digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.where(y_train == str(i))[0][0]\n",
    "    axes[i].imshow(X_train[idx].reshape(28, 28), cmap='gray')\n",
    "    axes[i].set_title(f\"Digit: {y_train[idx]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to MNIST\n",
    "n_components = 100  # We'll keep 100 components initially\n",
    "pca_mnist = PCA(n_components=n_components)\n",
    "X_train_pca = pca_mnist.fit_transform(X_train)\n",
    "X_test_pca = pca_mnist.transform(X_test)\n",
    "\n",
    "print(f\"Original dimensions: {X_train.shape[1]}\")\n",
    "print(f\"Reduced dimensions: {X_train_pca.shape[1]}\")\n",
    "print(f\"Dimensionality reduction: {X_train.shape[1]/X_train_pca.shape[1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance ratio\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Cumulative explained variance\n",
    "plt.plot(np.cumsum(pca_mnist.explained_variance_ratio_), marker='o', markersize=3)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('MNIST: Cumulative Explained Variance')\n",
    "\n",
    "# Add lines at 80%, 90%, 95% variance\n",
    "for threshold, color, label in zip([0.8, 0.9, 0.95], ['r', 'g', 'b'], ['80%', '90%', '95%']):\n",
    "    n_components = np.argmax(np.cumsum(pca_mnist.explained_variance_ratio_) >= threshold) + 1\n",
    "    plt.axhline(y=threshold, color=color, linestyle='--', alpha=0.5, label=f'{label} variance ({n_components} components)')\n",
    "    plt.axvline(x=n_components, color=color, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 25 principal components\n",
    "fig, axes = plt.subplots(5, 5, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(25):\n",
    "    component = pca_mnist.components_[i].reshape(28, 28)\n",
    "    axes[i].imshow(component, cmap='viridis')\n",
    "    axes[i].set_title(f\"PC {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"First 25 Principal Components of MNIST\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Reconstruction with PCA\n",
    "\n",
    "One interesting application of PCA is reconstructing the original data from the reduced dimensions. Let's see how the reconstructed MNIST digits look with different numbers of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reconstruct from PCA\n",
    "def reconstruct_from_pca(X, pca, n_components=None):\n",
    "    pca_obj = PCA(n_components=n_components)\n",
    "    pca_obj.fit(X)\n",
    "    X_reduced = pca_obj.transform(X)\n",
    "    X_reconstructed = pca_obj.inverse_transform(X_reduced)\n",
    "    return X_reconstructed, pca_obj.explained_variance_ratio_.sum()\n",
    "\n",
    "# Select a test example\n",
    "digit_idx = np.where(y_test == '5')[0][0]  # Find a '5' digit\n",
    "original_digit = X_test[digit_idx]\n",
    "\n",
    "# Reconstruct with different numbers of components\n",
    "n_components_list = [5, 10, 20, 50, 100, 200]\n",
    "reconstructed_digits = []\n",
    "explained_variances = []\n",
    "\n",
    "for n in n_components_list:\n",
    "    X_reconstructed, variance = reconstruct_from_pca(X_test, pca_mnist, n)\n",
    "    reconstructed_digits.append(X_reconstructed[digit_idx])\n",
    "    explained_variances.append(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original and reconstructed digits\n",
    "fig, axes = plt.subplots(1, len(n_components_list) + 1, figsize=(16, 3))\n",
    "\n",
    "# Plot original\n",
    "axes[0].imshow(original_digit.reshape(28, 28), cmap='gray')\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot reconstructions\n",
    "for i, (n, rec_digit, variance) in enumerate(zip(n_components_list, reconstructed_digits, explained_variances)):\n",
    "    axes[i+1].imshow(rec_digit.reshape(28, 28), cmap='gray')\n",
    "    axes[i+1].set_title(f\"{n} Components\\n{variance:.2%} Variance\")\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.suptitle(\"MNIST Digit Reconstruction with PCA\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA for Classification of MNIST\n",
    "\n",
    "Let's see how PCA can be used as a preprocessing step for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create pipelines with different numbers of PCA components\n",
    "component_options = [20, 50, 100, 200]\n",
    "results = []\n",
    "\n",
    "for n_components in component_options:\n",
    "    # Create pipeline with PCA and SVM\n",
    "    pipeline = Pipeline([\n",
    "        ('pca', PCA(n_components=n_components)),\n",
    "        ('svm', SVC(kernel='rbf', gamma='scale'))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append((n_components, accuracy))\n",
    "    print(f\"PCA with {n_components} components: {accuracy:.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "components, accuracies = zip(*results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(components, accuracies, marker='o', linestyle='-')\n",
    "plt.title('Classification Accuracy vs. Number of PCA Components')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA in a Machine Learning Pipeline\n",
    "\n",
    "Scikit-Learn's Pipeline makes it easy to include PCA as a preprocessing step in your machine learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with standardization, PCA, and a classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=100)),\n",
    "    ('classifier', SVC(kernel='rbf', gamma='scale'))\n",
    "])\n",
    "\n",
    "# Parameters to search\n",
    "param_grid = {\n",
    "    'pca__n_components': [50, 100, 150],\n",
    "    'classifier__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Tips for Using PCA\n",
    "\n",
    "### When to Use PCA\n",
    "\n",
    "- **High-dimensional data**: When you have many features (especially if many are correlated)\n",
    "- **Visualization**: To reduce dimensionality for visualization (2D or 3D plots)\n",
    "- **Noise reduction**: To filter out noise by discarding less important components\n",
    "- **Speed/efficiency**: To speed up machine learning algorithms\n",
    "- **Multicollinearity**: To address multicollinearity in regression problems\n",
    "\n",
    "### When Not to Use PCA\n",
    "\n",
    "- **Interpretability is critical**: PCA components are not easily interpretable\n",
    "- **Feature importance is needed**: PCA transforms features, making it hard to identify importance of original features\n",
    "- **Sparse data**: Other methods like sparse PCA might be more appropriate\n",
    "- **Non-linear relationships**: PCA captures linear relationships; consider kernel PCA or t-SNE for non-linear data\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Standardize data**: Always standardize/normalize data before applying PCA\n",
    "2. **Check variance explained**: Look at cumulative variance to determine number of components\n",
    "3. **Beware of outliers**: PCA is sensitive to outliers\n",
    "4. **Cross-validate**: When using PCA in a pipeline, cross-validate to find optimal parameters\n",
    "5. **Consider the problem**: For some problems, domain-specific feature engineering might be better than PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with Other Dimensionality Reduction Techniques\n",
    "\n",
    "PCA is just one of many dimensionality reduction techniques. Here's a brief comparison with other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PCA with t-SNE on MNIST\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Use a subset for speed\n",
    "n_samples = 2000\n",
    "X_subset = X_train[:n_samples]\n",
    "y_subset = y_train[:n_samples]\n",
    "\n",
    "# Apply PCA\n",
    "pca_vis = PCA(n_components=2)\n",
    "X_pca_vis = pca_vis.fit_transform(X_subset)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_subset)\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot PCA\n",
    "for digit in range(10):\n",
    "    idx = y_subset == str(digit)\n",
    "    axes[0].scatter(X_pca_vis[idx, 0], X_pca_vis[idx, 1], alpha=0.7, label=f\"Digit {digit}\")\n",
    "axes[0].set_title('PCA: MNIST Digits')\n",
    "axes[0].set_xlabel('Principal Component 1')\n",
    "axes[0].set_ylabel('Principal Component 2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot t-SNE\n",
    "for digit in range(10):\n",
    "    idx = y_subset == str(digit)\n",
    "    axes[1].scatter(X_tsne[idx, 0], X_tsne[idx, 1], alpha=0.7, label=f\"Digit {digit}\")\n",
    "axes[1].set_title('t-SNE: MNIST Digits')\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Dimensionality Reduction Techniques\n",
    "\n",
    "| Technique | Strengths | Weaknesses | Use Cases |\n",
    "|-----------|-----------|------------|------------|\n",
    "| **PCA** | Fast, simple, well-understood<br>Preserves global structure<br>Handles high-dimensional data | Linear relationships only<br>Sensitive to scaling<br>Components not easily interpretable | General dimensionality reduction<br>Preprocessing step<br>Noise reduction |\n",
    "| **t-SNE** | Preserves local structure<br>Better separation of clusters<br>Works well for visualization | Slow on large datasets<br>Stochastic results<br>Not suitable for preprocessing | Visualization<br>Exploring cluster structure<br>Non-linear data |\n",
    "| **UMAP** | Faster than t-SNE<br>Preserves global & local structure<br>Theoretical foundation | Complex parameter tuning<br>Newer, less well-understood | Visualization<br>Alternative to t-SNE<br>Large datasets |\n",
    "| **Kernel PCA** | Captures non-linear relationships<br>Extends PCA framework<br>Theoretical foundation | Computationally expensive<br>Difficult kernel selection<br>Hard to interpret | Non-linear data<br>When linear PCA fails |\n",
    "| **Factor Analysis** | Statistical model<br>Focuses on correlations<br>Good for latent factors | Assumes linear relationships<br>Needs distributional assumptions | Social sciences<br>Psychology<br>Market research |\n",
    "| **Autoencoders** | Very flexible<br>Can capture complex patterns<br>Non-linear capabilities | Requires neural network expertise<br>Computationally expensive<br>Risk of overfitting | Complex data<br>Image processing<br>Deep learning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. The fundamentals of PCA and dimensionality reduction\n",
    "2. How to use Scikit-Learn's PCA implementation\n",
    "3. The importance of explained variance ratio in selecting components\n",
    "4. Randomized PCA for large datasets\n",
    "5. Incremental PCA for out-of-memory data processing\n",
    "6. Application of PCA to the MNIST dataset\n",
    "7. Including PCA in a machine learning pipeline\n",
    "8. Practical tips for using PCA effectively\n",
    "9. Comparison with other dimensionality reduction techniques\n",
    "\n",
    "Principal Component Analysis is a powerful technique for dimensionality reduction with applications across many domains. It provides a balance of simplicity, efficiency, and effectiveness that makes it a standard tool in any data scientist's toolkit.\n",
    "\n",
    "Remember, while PCA is excellent for many applications, it's always important to consider the specific needs of your problem and data to determine if it's the right tool for the job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
